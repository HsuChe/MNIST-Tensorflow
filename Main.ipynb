{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scale(image,label):\n",
    "    # changes the image data type to float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # input the range 0-255 that a pixel can be for the dataset\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "def keras_sequence (hidden_layer_size ,depth,layer_activation):\n",
    "    sequential = [tf.keras.layers.Flatten(input_shape=(28,28,1))]\n",
    "    for i in range(depth):\n",
    "        sequential.append(tf.keras.layers.Dense(hidden_layer_size, activation=layer_activation))\n",
    "    sequential.append(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    return sequential\n",
    "\n",
    "def MNIST_Train( \n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation\n",
    "    ):\n",
    "    # load the dataset and the info from tensorflow_dataset 'mnist'\n",
    "    mnist_dataset, mnist_info = tfds.load(name = 'mnist',with_info = True, as_supervised=True)\n",
    "    \n",
    "    # set the train dataset and the test dataset\n",
    "    mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "    \n",
    "    # set the number of data you want to take from training for validation dataset\n",
    "    num_validation_sample = mnist_info.splits['train'].num_examples*validation_per\n",
    "    # making sure that number is an int type\n",
    "    num_validation_sample = tf.cast(num_validation_sample,tf.int64)\n",
    "    \n",
    "    # set the number of data you want to take for tests whcih is just the length of the test dataset\n",
    "    num_test_sample = mnist_info.splits['test'].num_examples\n",
    "    # making sure the number is an int type\n",
    "    num_test_sample = tf.cast(num_test_sample,tf.int64)\n",
    "    \n",
    "    # scale the train dataset and the test data set with the scale function\n",
    "    scaled_train = mnist_train.map(scale)\n",
    "    test_data = mnist_test.map(scale)\n",
    "    \n",
    "    # now we need to shuffle the data to make sure the sequence of the data is random\n",
    "    # we use the method .shuffle with specific buffer size for the task\n",
    "    shuffled_train_dataset = scaled_train.shuffle(BUFFER_SIZE)\n",
    "    \n",
    "    # now we extract the validation dataset from train dataset and remove validation dataset from train dataset\n",
    "    validation_data = shuffled_train_dataset.take(num_validation_sample)\n",
    "    train_data = shuffled_train_dataset.skip(num_validation_sample)\n",
    "    \n",
    "    # now we batch up the data so it doesn't crash the computer if it is a big dataset\n",
    "    train_data = train_data.batch(BATCH_SIZE)\n",
    "    \n",
    "    # we set the amount of data we want to batch for validation dataset to be the same as the length of validation dataset.\n",
    "    validation_data = validation_data.batch(num_validation_sample)\n",
    "    \n",
    "    # we set the amount of data we want to batch for test dataset to be the same as the length of test dataset.\n",
    "    test_data = test_data.batch(num_test_sample)\n",
    "    \n",
    "    # we now iterate the next batch until all of the dataset is used.\n",
    "    validation_inputs, validation_targets = next(iter(validation_data))\n",
    "    \n",
    "    # we formate the keras_sequence that will be the input into the model\n",
    "    sequential = keras_sequence(hidden_layer_size,depth,layer_activation)\n",
    "    # set the right sequential to the model\n",
    "    model = tf.keras.Sequential(sequential)\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
    "    \n",
    "    # fit the model according to the number of EPOCHS\n",
    "    model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs,validation_targets), verbose = 2)\n",
    "    \n",
    "    return model, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 4s - loss: 0.2724 - accuracy: 0.9209 - val_loss: 0.1492 - val_accuracy: 0.9538\n",
      "Epoch 2/5\n",
      "540/540 - 2s - loss: 0.1053 - accuracy: 0.9685 - val_loss: 0.0872 - val_accuracy: 0.9725\n",
      "Epoch 3/5\n",
      "540/540 - 2s - loss: 0.0703 - accuracy: 0.9780 - val_loss: 0.0581 - val_accuracy: 0.9823\n",
      "Epoch 4/5\n",
      "540/540 - 2s - loss: 0.0536 - accuracy: 0.9835 - val_loss: 0.0509 - val_accuracy: 0.9837\n",
      "Epoch 5/5\n",
      "540/540 - 2s - loss: 0.0418 - accuracy: 0.9866 - val_loss: 0.0546 - val_accuracy: 0.9838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.sequential.Sequential at 0x1e2e04a6c10>,\n",
       " <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjusting hidden layer size to 200\n",
    "validation_per = 0.1\n",
    "optimizer = 'adam'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "metrics = 'accuracy'\n",
    "hidden_layer_size = 200\n",
    "depth = 2\n",
    "layer_activation = 'relu'\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "MNIST_Train(\n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss rom adjusting the hidden layer size by 2x incrased accuracy by around 0.002\n",
    "# similar amount of rendering time and the loss from the early epochs were much lower\n",
    "# might be able to reduce epochs to achieve similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 4s - loss: 0.2660 - accuracy: 0.9196 - val_loss: 0.1326 - val_accuracy: 0.9617\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.1022 - accuracy: 0.9682 - val_loss: 0.0883 - val_accuracy: 0.9742\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0696 - accuracy: 0.9782 - val_loss: 0.0611 - val_accuracy: 0.9793\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0537 - accuracy: 0.9829 - val_loss: 0.0587 - val_accuracy: 0.9815\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0401 - accuracy: 0.9875 - val_loss: 0.0532 - val_accuracy: 0.9847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.sequential.Sequential at 0x1e2e09432e0>,\n",
       " <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjusting depth from 2 to 3\n",
    "depth = 3\n",
    "\n",
    "MNIST_Train(\n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting the depth by adding a layer from 2 to 3 improved accuracy by 0.006 which without a big hit to time\n",
    "# also improves the losses of early epochs by a good amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 5s - loss: 0.3208 - accuracy: 0.9035 - val_loss: 0.1514 - val_accuracy: 0.9570\n",
      "Epoch 2/5\n",
      "540/540 - 4s - loss: 0.1358 - accuracy: 0.9614 - val_loss: 0.1142 - val_accuracy: 0.9692\n",
      "Epoch 3/5\n",
      "540/540 - 4s - loss: 0.0996 - accuracy: 0.9724 - val_loss: 0.0871 - val_accuracy: 0.9767\n",
      "Epoch 4/5\n",
      "540/540 - 4s - loss: 0.0851 - accuracy: 0.9768 - val_loss: 0.0867 - val_accuracy: 0.9780\n",
      "Epoch 5/5\n",
      "540/540 - 4s - loss: 0.0703 - accuracy: 0.9809 - val_loss: 0.0760 - val_accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.sequential.Sequential at 0x1e2e717e190>,\n",
       " <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing depth to 10\n",
    "depth = 10\n",
    "\n",
    "MNIST_Train(\n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by adding additional depth, the accuracy went down instead of increasing, suggesting that we might be overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 5s - loss: 0.2730 - accuracy: 0.9165 - val_loss: 0.1169 - val_accuracy: 0.9675\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.1097 - accuracy: 0.9667 - val_loss: 0.0857 - val_accuracy: 0.9738\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0765 - accuracy: 0.9762 - val_loss: 0.0869 - val_accuracy: 0.9728\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0585 - accuracy: 0.9815 - val_loss: 0.0527 - val_accuracy: 0.9847\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0519 - accuracy: 0.9838 - val_loss: 0.0699 - val_accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.sequential.Sequential at 0x1e2ec68a070>,\n",
       " <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth = 5\n",
    "\n",
    "MNIST_Train(\n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 4s - loss: 0.9286 - accuracy: 0.6876 - val_loss: 0.3647 - val_accuracy: 0.8947\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.2927 - accuracy: 0.9182 - val_loss: 0.2180 - val_accuracy: 0.9380\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.1973 - accuracy: 0.9449 - val_loss: 0.1542 - val_accuracy: 0.9582\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.1486 - accuracy: 0.9573 - val_loss: 0.1392 - val_accuracy: 0.9588\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.1197 - accuracy: 0.9651 - val_loss: 0.0954 - val_accuracy: 0.9727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.sequential.Sequential at 0x1e2e5c37a60>,\n",
       " <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_activation = 'sigmoid'\n",
    "\n",
    "MNIST_Train(\n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 4s - loss: 0.2810 - accuracy: 0.9143 - val_loss: 0.1549 - val_accuracy: 0.9547\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.1413 - accuracy: 0.9555 - val_loss: 0.1280 - val_accuracy: 0.9612\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.1000 - accuracy: 0.9698 - val_loss: 0.0967 - val_accuracy: 0.9702\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0785 - accuracy: 0.9749 - val_loss: 0.0930 - val_accuracy: 0.9700\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0602 - accuracy: 0.9810 - val_loss: 0.0696 - val_accuracy: 0.9790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.sequential.Sequential at 0x1e2e6115460>,\n",
       " <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_activation = 'tanh'\n",
    "\n",
    "MNIST_Train(\n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6/6 - 3s - loss: 1.6141 - accuracy: 0.5220 - val_loss: 0.8075 - val_accuracy: 0.7987\n",
      "Epoch 2/5\n",
      "6/6 - 2s - loss: 0.6455 - accuracy: 0.8289 - val_loss: 0.4667 - val_accuracy: 0.8690\n",
      "Epoch 3/5\n",
      "6/6 - 2s - loss: 0.4187 - accuracy: 0.8794 - val_loss: 0.3660 - val_accuracy: 0.8905\n",
      "Epoch 4/5\n",
      "6/6 - 2s - loss: 0.3427 - accuracy: 0.8983 - val_loss: 0.3169 - val_accuracy: 0.9052\n",
      "Epoch 5/5\n",
      "6/6 - 2s - loss: 0.3022 - accuracy: 0.9104 - val_loss: 0.2852 - val_accuracy: 0.9163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.sequential.Sequential at 0x1e2e65f2eb0>,\n",
       " <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 10000\n",
    "\n",
    "MNIST_Train(\n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5400/5400 - 11s - loss: 0.3056 - accuracy: 0.9081 - val_loss: 0.1940 - val_accuracy: 0.9475\n",
      "Epoch 2/5\n",
      "5400/5400 - 10s - loss: 0.1741 - accuracy: 0.9499 - val_loss: 0.1445 - val_accuracy: 0.9623\n",
      "Epoch 3/5\n",
      "5400/5400 - 10s - loss: 0.1339 - accuracy: 0.9613 - val_loss: 0.1455 - val_accuracy: 0.9607\n",
      "Epoch 4/5\n",
      "5400/5400 - 9s - loss: 0.1129 - accuracy: 0.9678 - val_loss: 0.1176 - val_accuracy: 0.9675\n",
      "Epoch 5/5\n",
      "5400/5400 - 9s - loss: 0.0987 - accuracy: 0.9712 - val_loss: 0.1078 - val_accuracy: 0.9720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.sequential.Sequential at 0x1e28009e0d0>,\n",
       " <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "MNIST_Train(\n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 4s - loss: 0.2726 - accuracy: 0.9190 - val_loss: 0.1284 - val_accuracy: 0.9605\n",
      "Epoch 2/5\n",
      "540/540 - 2s - loss: 0.1027 - accuracy: 0.9685 - val_loss: 0.0900 - val_accuracy: 0.9703\n",
      "Epoch 3/5\n",
      "540/540 - 2s - loss: 0.0693 - accuracy: 0.9784 - val_loss: 0.0739 - val_accuracy: 0.9783\n",
      "Epoch 4/5\n",
      "540/540 - 2s - loss: 0.0535 - accuracy: 0.9831 - val_loss: 0.0475 - val_accuracy: 0.9843\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0441 - accuracy: 0.9855 - val_loss: 0.0565 - val_accuracy: 0.9827\n"
     ]
    }
   ],
   "source": [
    "# The best model seems to be with hidden_layer_size of 200 and depth of 3 while everything else stays the same\n",
    "\n",
    "name = 'mnist'\n",
    "validation_per = 0.1\n",
    "optimizer = 'adam'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "metrics = 'accuracy'\n",
    "hidden_layer_size = 200\n",
    "depth = 3\n",
    "layer_activation = 'relu'\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "model, test_data = MNIST_Train(\n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 613ms/step - loss: 0.0823 - accuracy: 0.9755\n",
      "Test loss: 0.08. Test accuracy: 97.55%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model has a accuracy of 97.94% in deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3-TF2.0)",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
