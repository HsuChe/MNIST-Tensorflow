{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Tensor Flow Deep Neural Netowrk Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is called MNIST and refers to handwritten digital recognition. You can find more about it on Yann LeCun's website. \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image)\n",
    "\n",
    "The goal is to write an algorithm that detects whcih digit is written. Since there are only 10 digits (0,1,2,3,4,5,6,7,8,9) this is a classification problem with 10 classes\n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the relevent packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x1f\\x8b\\x08\\x08z\\x82\\x902\\x00\\x03train-images.idx3-ubyte\\x00\\xec\\x9c\\x07T\\x15G\\xdb\\xc7\\'\\x01\\xc4\\x06\"v\\x13\\x04\\x15\\xc4\\x86B\\x8a5*F\\xf3&\\xc6\\xa8\\x18[l\\x115\\x891\\xc6\\x96\\xa8\\t\\xbe\\xf6\\x12{4\\xc6^\\xf0\\x8d\\xb1\\x17\\x12\\x01[>\\xc1\\x8a\\x15D\\x05\\x11\\x94^\\xa4*\\x9d\\x0b'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST_path = 'train-images-idx3-ubyte.gz'\n",
    "gzip_file = open(MNIST_path, mode='rb')\n",
    "    #Shorthand for GzipFile(filename, mode, compresslevel).\n",
    "MNIST_sample = gzip_file.read()\n",
    "MNIST_sample[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name = 'mnist', with_info=True, as_supervised = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'],mnist_dataset['test']\n",
    "# from here you want to find the validation sample, the test sample, and the train sample\n",
    "# we will start with the validation sample\n",
    "# the mnist_info.splits is purely for the dataset size\n",
    "num_validation_sample = mnist_info.splits['train'].num_examples*0.1\n",
    "# now we run it through tf.cast which changest the type of the tensor and choose int64\n",
    "num_validation_sample = tf.cast(num_validation_sample,tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can clarify the test and validation sample\n",
    "num_test_samples = tf.cast(mnist_info.splits['test'].num_examples,tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would like now to scale the information so that it is more numerically stable\n",
    "# we can write a stablization funtion and apply to the .map method to transform the data\n",
    "\n",
    "def scale (image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now run the scale function for both the train and test data\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to shuffle the data for validation which will help make the data more accurate\n",
    "\n",
    "# this is used in cases with a lot of data as we won't be able to shuffle the entire data in one go\n",
    "# we won't be able to fit the data in the system memory in one go\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# now we need to shuffle\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "# after everything is shuffled, we can now take what we need for validation data\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_sample)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_sample)\n",
    "\n",
    "# now we have the batch up the data\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_sample)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# now we can command to take the next batch\n",
    "# we got a 2 tuple structure beacause supervised = True means we have a target\n",
    "\n",
    "validation_inputs,validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)> <BatchDataset shapes: ((None, None, None, 28, 28, 1), (None, None, None)), types: (tf.float32, tf.int64)> <BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)> (6000, 28, 28, 1) (6000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data, test_data,validation_data,validation_inputs.shape, validation_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_sequence (input_shape,output_size,hidden_layer_size,depth,layer_activation,output_activation):\n",
    "    sequential = [tf.keras.layers.Flatten(input_shape=input_shape)]\n",
    "    for i in range(depth):\n",
    "        sequential.append(tf.keras.layers.Dense(hidden_layer_size, activation=layer_activation))\n",
    "    sequential.append(tf.keras.layers.Dense(output_size, activation=output_activation))\n",
    "    return sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28,28,1)\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "depth = 2\n",
    "layer_activation = 'relu'\n",
    "# transforming the values into a probability for the output function we use 'softmax'\n",
    "output_activation = 'softmax'\n",
    "\n",
    "sequential = keras_sequence(input_shape,output_size,hidden_layer_size,depth,layer_activation,output_activation)\n",
    "model = tf.keras.Sequential(sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x1ec051699a0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1ec0514e1f0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1ec05178460>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1ec05178880>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choosing the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using adaptive moment estimator\n",
    "optimizer = 'adam'\n",
    "# used for classifiers, cross entropy is a good choice, but there are 3 loss functions\n",
    "# binary cross entropy, categorical cross entropy, sparse categorical cross entropy\n",
    "# output shape and target shape should match, so cross entropy is good, need to find out what \"one hot encoding\" means\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "# metrics we want throughout the training process such as accuracy\n",
    "metrics = 'accuracy'\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 4s - loss: 0.4101 - accuracy: 0.8831 - val_loss: 0.2029 - val_accuracy: 0.9375\n",
      "Epoch 2/5\n",
      "540/540 - 2s - loss: 0.1812 - accuracy: 0.9464 - val_loss: 0.1465 - val_accuracy: 0.9553\n",
      "Epoch 3/5\n",
      "540/540 - 2s - loss: 0.1399 - accuracy: 0.9592 - val_loss: 0.1398 - val_accuracy: 0.9540\n",
      "Epoch 4/5\n",
      "540/540 - 2s - loss: 0.1143 - accuracy: 0.9664 - val_loss: 0.1038 - val_accuracy: 0.9693\n",
      "Epoch 5/5\n",
      "540/540 - 2s - loss: 0.0971 - accuracy: 0.9712 - val_loss: 0.0934 - val_accuracy: 0.9708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec05199430>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs,validation_targets), verbose = 2)\n",
    "# here is what will happen\n",
    "# at the beginning of each EPOCH, the training loss will be 0\n",
    "# the algorithm will iterate over a preset number of batches, all from train_data\n",
    "# then the weights and biases will be updated basedo on the number of batches\n",
    "# we will get a value for hte loss function, indicating the status of the training\n",
    "# we will see a training accuracy\n",
    "# at the end of the EPOCH, it will forward propagate with the validation dataset\n",
    "# and it'll repeat until the set EPOCH is reached \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3-TF2.0)",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
