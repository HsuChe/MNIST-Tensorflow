{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scale(image,label):\n",
    "    # changes the image data type to float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # input the range 0-255 that a pixel can be for the dataset\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "def keras_sequence (hidden_layer_size ,depth,layer_activation):\n",
    "    sequential = [tf.keras.layers.Flatten(input_shape=(28,28,1))]\n",
    "    for i in range(depth):\n",
    "        sequential.append(tf.keras.layers.Dense(hidden_layer_size, activation=layer_activation))\n",
    "    sequential.append(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    return sequential\n",
    "\n",
    "def MNIST_Train(\n",
    "    name, \n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation\n",
    "    ):\n",
    "    # load the dataset and the info from tensorflow_dataset 'mnist'\n",
    "    mnist_dataset, mnist_info = tfds.load(name = name,with_info = True, as_supervised=True)\n",
    "    \n",
    "    # set the train dataset and the test dataset\n",
    "    mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "    \n",
    "    # set the number of data you want to take from training for validation dataset\n",
    "    num_validation_sample = mnist_info.splits['train'].num_examples*validation_per\n",
    "    # making sure that number is an int type\n",
    "    num_validation_sample = tf.cast(num_validation_sample,tf.int64)\n",
    "    \n",
    "    # set the number of data you want to take for tests whcih is just the length of the test dataset\n",
    "    num_test_sample = mnist_info.splits['test'].num_examples\n",
    "    # making sure the number is an int type\n",
    "    num_test_sample = tf.cast(num_test_sample,tf.int64)\n",
    "    \n",
    "    # scale the train dataset and the test data set with the scale function\n",
    "    scaled_train = mnist_train.map(scale)\n",
    "    test_data = mnist_test.map(scale)\n",
    "    \n",
    "    # now we need to shuffle the data to make sure the sequence of the data is random\n",
    "    # we use the method .shuffle with specific buffer size for the task\n",
    "    shuffled_train_dataset = scaled_train.shuffle(BUFFER_SIZE)\n",
    "    \n",
    "    # now we extract the validation dataset from train dataset and remove validation dataset from train dataset\n",
    "    validation_data = shuffled_train_dataset.take(num_validation_sample)\n",
    "    train_data = shuffled_train_dataset.skip(num_validation_sample)\n",
    "    \n",
    "    # now we batch up the data so it doesn't crash the computer if it is a big dataset\n",
    "    train_data = train_data.batch(BATCH_SIZE)\n",
    "    \n",
    "    # we set the amount of data we want to batch for validation dataset to be the same as the length of validation dataset.\n",
    "    validation_data = validation_data.batch(num_validation_sample)\n",
    "    \n",
    "    # we set the amount of data we want to batch for test dataset to be the same as the length of test dataset.\n",
    "    test_data = test_data.batch(num_test_sample)\n",
    "    \n",
    "    # we now iterate the next batch until all of the dataset is used.\n",
    "    validation_inputs, validation_targets = next(iter(validation_data))\n",
    "    \n",
    "    # we formate the keras_sequence that will be the input into the model\n",
    "    sequential = keras_sequence(hidden_layer_size,depth,layer_activation)\n",
    "    # set the right sequential to the model\n",
    "    model = tf.keras.Sequential(sequential)\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
    "    \n",
    "    # fit the model according to the number of EPOCHS\n",
    "    model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs,validation_targets), verbose = 2)\n",
    "    \n",
    "    return model, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 4s - loss: 0.2804 - accuracy: 0.9170 - val_loss: 0.1339 - val_accuracy: 0.9607\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.1080 - accuracy: 0.9676 - val_loss: 0.0816 - val_accuracy: 0.9758\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0728 - accuracy: 0.9773 - val_loss: 0.0699 - val_accuracy: 0.9777\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0526 - accuracy: 0.9839 - val_loss: 0.0578 - val_accuracy: 0.9822\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0421 - accuracy: 0.9870 - val_loss: 0.0446 - val_accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "# adjusting hidden layer size to 200\n",
    "name = 'mnist'\n",
    "validation_per = 0.1\n",
    "optimizer = 'adam'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "metrics = 'accuracy'\n",
    "hidden_layer_size = 200\n",
    "depth = 2\n",
    "layer_activation = 'relu'\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "MNIST_Train(\n",
    "    name, \n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss rom adjusting the hidden layer size by 2x incrased accuracy by around 0.002\n",
    "# similar amount of rendering time and the loss from the early epochs were much lower\n",
    "# might be able to reduce epochs to achieve similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 4s - loss: 0.2691 - accuracy: 0.9208 - val_loss: 0.1388 - val_accuracy: 0.9592\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.0990 - accuracy: 0.9691 - val_loss: 0.0989 - val_accuracy: 0.9715\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0671 - accuracy: 0.9787 - val_loss: 0.0727 - val_accuracy: 0.9768\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0512 - accuracy: 0.9838 - val_loss: 0.0619 - val_accuracy: 0.9797\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0431 - accuracy: 0.9861 - val_loss: 0.0459 - val_accuracy: 0.9863\n"
     ]
    }
   ],
   "source": [
    "# adjusting depth from 2 to 3\n",
    "depth = 3\n",
    "\n",
    "MNIST_Train(\n",
    "    name, \n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting the depth by adding a layer from 2 to 3 improved accuracy by 0.006 which without a big hit to time\n",
    "# also improves the losses of early epochs by a good amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 6s - loss: 0.3416 - accuracy: 0.8948 - val_loss: 0.1640 - val_accuracy: 0.9562\n",
      "Epoch 2/5\n",
      "540/540 - 4s - loss: 0.1417 - accuracy: 0.9609 - val_loss: 0.1152 - val_accuracy: 0.9653\n",
      "Epoch 3/5\n",
      "540/540 - 5s - loss: 0.1045 - accuracy: 0.9703 - val_loss: 0.0936 - val_accuracy: 0.9758\n",
      "Epoch 4/5\n",
      "540/540 - 4s - loss: 0.0817 - accuracy: 0.9769 - val_loss: 0.1110 - val_accuracy: 0.9693\n",
      "Epoch 5/5\n",
      "540/540 - 5s - loss: 0.0719 - accuracy: 0.9794 - val_loss: 0.0754 - val_accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "# changing depth to 10\n",
    "depth = 10\n",
    "\n",
    "MNIST_Train(\n",
    "    name, \n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by adding additional depth, the accuracy went down instead of increasing, suggesting that we might be overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 5s - loss: 0.2739 - accuracy: 0.9175 - val_loss: 0.1399 - val_accuracy: 0.9567\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.1108 - accuracy: 0.9658 - val_loss: 0.1102 - val_accuracy: 0.9657\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0782 - accuracy: 0.9758 - val_loss: 0.0920 - val_accuracy: 0.9725\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0631 - accuracy: 0.9808 - val_loss: 0.0763 - val_accuracy: 0.9795\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0505 - accuracy: 0.9841 - val_loss: 0.0618 - val_accuracy: 0.9817\n"
     ]
    }
   ],
   "source": [
    "depth = 5\n",
    "\n",
    "MNIST_Train(\n",
    "    name, \n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 5s - loss: 1.0088 - accuracy: 0.6529 - val_loss: 0.3771 - val_accuracy: 0.8973\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.2928 - accuracy: 0.9182 - val_loss: 0.2275 - val_accuracy: 0.9355\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.2024 - accuracy: 0.9425 - val_loss: 0.1588 - val_accuracy: 0.9530\n",
      "Epoch 4/5\n",
      "540/540 - 4s - loss: 0.1520 - accuracy: 0.9566 - val_loss: 0.1408 - val_accuracy: 0.9588\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.1214 - accuracy: 0.9648 - val_loss: 0.1039 - val_accuracy: 0.9705\n"
     ]
    }
   ],
   "source": [
    "layer_activation = 'sigmoid'\n",
    "\n",
    "MNIST_Train(\n",
    "    name, \n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 5s - loss: 0.2840 - accuracy: 0.9135 - val_loss: 0.1619 - val_accuracy: 0.9523\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.1340 - accuracy: 0.9587 - val_loss: 0.1263 - val_accuracy: 0.9640\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0996 - accuracy: 0.9690 - val_loss: 0.0925 - val_accuracy: 0.9717\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0766 - accuracy: 0.9756 - val_loss: 0.0723 - val_accuracy: 0.9783\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0632 - accuracy: 0.9795 - val_loss: 0.0676 - val_accuracy: 0.9782\n"
     ]
    }
   ],
   "source": [
    "layer_activation = 'tanh'\n",
    "\n",
    "MNIST_Train(\n",
    "    name, \n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6/6 - 4s - loss: 1.5524 - accuracy: 0.5456 - val_loss: 0.7355 - val_accuracy: 0.8020\n",
      "Epoch 2/5\n",
      "6/6 - 2s - loss: 0.5966 - accuracy: 0.8351 - val_loss: 0.4311 - val_accuracy: 0.8775\n",
      "Epoch 3/5\n",
      "6/6 - 2s - loss: 0.3953 - accuracy: 0.8850 - val_loss: 0.3390 - val_accuracy: 0.8998\n",
      "Epoch 4/5\n",
      "6/6 - 2s - loss: 0.3269 - accuracy: 0.9030 - val_loss: 0.2938 - val_accuracy: 0.9103\n",
      "Epoch 5/5\n",
      "6/6 - 2s - loss: 0.2879 - accuracy: 0.9138 - val_loss: 0.2630 - val_accuracy: 0.9240\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10000\n",
    "\n",
    "MNIST_Train(\n",
    "    name, \n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5400/5400 - 12s - loss: 0.3091 - accuracy: 0.9081 - val_loss: 0.2093 - val_accuracy: 0.9403\n",
      "Epoch 2/5\n",
      "5400/5400 - 11s - loss: 0.1730 - accuracy: 0.9501 - val_loss: 0.1409 - val_accuracy: 0.9578\n",
      "Epoch 3/5\n",
      "5400/5400 - 12s - loss: 0.1359 - accuracy: 0.9614 - val_loss: 0.1304 - val_accuracy: 0.9617\n",
      "Epoch 4/5\n",
      "5400/5400 - 11s - loss: 0.1117 - accuracy: 0.9677 - val_loss: 0.1132 - val_accuracy: 0.9702\n",
      "Epoch 5/5\n",
      "5400/5400 - 12s - loss: 0.1005 - accuracy: 0.9713 - val_loss: 0.1048 - val_accuracy: 0.9693\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "MNIST_Train(\n",
    "    name, \n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 4s - loss: 0.2614 - accuracy: 0.9233 - val_loss: 0.1323 - val_accuracy: 0.9613\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.1026 - accuracy: 0.9688 - val_loss: 0.0849 - val_accuracy: 0.9760\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0705 - accuracy: 0.9779 - val_loss: 0.0587 - val_accuracy: 0.9825\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0543 - accuracy: 0.9829 - val_loss: 0.0631 - val_accuracy: 0.9792\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0398 - accuracy: 0.9871 - val_loss: 0.0390 - val_accuracy: 0.9890\n"
     ]
    }
   ],
   "source": [
    "# The best model seems to be with hidden_layer_size of 200 and depth of 3 while everything else stays the same\n",
    "\n",
    "name = 'mnist'\n",
    "validation_per = 0.1\n",
    "optimizer = 'adam'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "metrics = 'accuracy'\n",
    "hidden_layer_size = 200\n",
    "depth = 3\n",
    "layer_activation = 'relu'\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "model, test_data = MNIST_Train(\n",
    "    name, \n",
    "    validation_per, \n",
    "    BUFFER_SIZE, \n",
    "    BATCH_SIZE,\n",
    "    hidden_layer_size,\n",
    "    depth,\n",
    "    layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 520ms/step - loss: 0.0733 - accuracy: 0.9794\n",
      "Test loss: 0.07. Test accuracy: 97.94%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model has a accuracy of 97.94% in deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3-TF2.0)",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
